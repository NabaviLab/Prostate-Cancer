{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e38a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "#from dataset import Dataset, collate_fn\n",
    "SEED = 2333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41a8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "#feature_dir = 'Feature_vector//Feature_vector//'\n",
    "graph_dir = 'tg//'\n",
    "label_dir = 'tma_l//'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folders = os.listdir(graph_dir)\n",
    "sequence_features = []\n",
    "sequence_graphs = []\n",
    "labels = []\n",
    "sequence_names=[]\n",
    "for i in range(len(folders)):\n",
    "    sequence_features.append(pickle.load(open(graph_dir + folders[i] +  '//graph.pkl'  , \"rb\"))['node_features'])\n",
    "    sequence_graphs.append(pickle.load(open(graph_dir + folders[i] +  '//graph.pkl'  , \"rb\"))['Adjacency_matrix'])\n",
    "    labels.append(pickle.load(open(label_dir + folders[i] +  '//label.pkl'  , \"rb\"))[0][-1])\n",
    "    sequence_names.append(folders[i])\n",
    "    \n",
    "\n",
    "#vectors = pickle.load(open(feature_dir + folders[i] +  '//vectors.pkl'  , \"rb\"))\n",
    "\n",
    "#labels = pickle.load(open(label_dir + folders[i] +  '//label.pkl'  , \"rb\"))[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff5124a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Path = './tm/'\n",
    "Result_Path = './tm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27552afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MG = dict(zip(sequence_names, sequence_graphs))\n",
    "WF=dict(zip(sequence_names, sequence_features))\n",
    "zipped = list(zip(sequence_names, labels))\n",
    "ds = pd.DataFrame(zipped, columns=['names', 'stability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa6878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NodeM(name):\n",
    "    return WF[name]\n",
    "def GraphM(name):\n",
    "    return MG[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2471549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ds['names'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9fa3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "  \n",
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.names = dataframe['names'].values.tolist()\n",
    "        self.labels = dataframe['stability'].values.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sequence_name = self.names[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        sequence_feature = NodeM(sequence_name)\n",
    "\n",
    "        # L * L\n",
    "        sequence_graph = GraphM(sequence_name)\n",
    "        \n",
    "        \n",
    "        sample = {'sequence_feature': sequence_feature,\\\n",
    "                  'sequence_graph': sequence_graph, \\\n",
    "                  'label': label, \\\n",
    "                  'sequence_name': sequence_name, \\\n",
    "                  }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequence_feature = []\n",
    "    sequence_graph = []\n",
    "    sequence_names = [] \n",
    "    labels=[]   \n",
    "    for i in range(len(batch)):\n",
    "        sequence_feature.append(batch[i]['sequence_feature'])\n",
    "        sequence_feature=np.asarray(sequence_feature)\n",
    "        sequence_graph.append(batch[i]['sequence_graph'])\n",
    "        sequence_graph=np.asarray(sequence_graph)\n",
    "        sequence_names.append(batch[i]['sequence_name'])\n",
    "        labels.append(batch[i]['label'])\n",
    "        labels= np.asarray(labels)\n",
    "\n",
    "    sequence_feature = torch.from_numpy(sequence_feature).float()\n",
    "    sequence_graph = torch.from_numpy(sequence_graph).float()\n",
    "    labels= torch.from_numpy(labels)\n",
    "\n",
    "    return sequence_feature,sequence_graph, labels, sequence_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb9fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_EPOCHS = 150\n",
    "LEARNING_RATE = 1E-5\n",
    "WEIGHT_DECAY = 10E-7\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "# GCN parameters\n",
    "GCN_FEATURE_DIM = 2560\n",
    "GCN_HIDDEN_DIM = 512\n",
    "GCN_HIDDEN_DIM1 = 256\n",
    "GCN_HIDDEN_DIM2 = 128\n",
    "GCN_OUTPUT_DIM = 32\n",
    "\n",
    "# Attention parameters\n",
    "DENSE_DIM = 16\n",
    "ATTENTION_HEADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12ed3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "#from utils.utils import initialize_weights\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae6fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = input @ self.weight    # X * W\n",
    "        output = adj @ support           # A * X * W\n",
    "        if self.bias is not None:        # A * X * W + b\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(GCN_FEATURE_DIM, GCN_HIDDEN_DIM)\n",
    "        self.ln1 = nn.LayerNorm(GCN_HIDDEN_DIM)\n",
    "        self.gc3 = GraphConvolution(GCN_HIDDEN_DIM, GCN_HIDDEN_DIM1)\n",
    "        self.ln3 = nn.LayerNorm(GCN_HIDDEN_DIM1)\n",
    "        self.gc2 = GraphConvolution(GCN_HIDDEN_DIM1, GCN_OUTPUT_DIM)\n",
    "        self.ln2 = nn.LayerNorm(GCN_OUTPUT_DIM)\n",
    "        self.relu1 = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.relu3 = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.relu4 = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.relu2 = nn.LeakyReLU(0.2,inplace=True)\n",
    "\n",
    "    def forward(self, x, adj):  \t\t\t# x.shape = (seq_len, GCN_FEATURE_DIM); adj.shape = (seq_len, seq_len)\n",
    "        x = self.gc1(x, adj)  \t\t\t\t# x.shape = (seq_len, GCN_HIDDEN_DIM)\n",
    "        x = self.relu1(self.ln1(x))\n",
    "        x = self.gc3(x, adj)  \t\t\t\t# x.shape = (seq_len, GCN_HIDDEN_DIM)\n",
    "        x = self.relu3(self.ln3(x))\n",
    "        x = self.gc2(x, adj)\n",
    "        output = self.relu2(self.ln2(x))\t# output.shape = (seq_len, GCN_OUTPUT_DIM)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, dense_dim, n_heads):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.dense_dim)\n",
    "        self.fc2 = nn.Linear(self.dense_dim, self.n_heads)\n",
    "\n",
    "    def softmax(self, input, axis=1):\n",
    "        input_size = input.size()\n",
    "        trans_input = input.transpose(axis, len(input_size) - 1)\n",
    "        trans_size = trans_input.size()\n",
    "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "        soft_max_2d = torch.softmax(input_2d, dim=1)\n",
    "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "        return soft_max_nd.transpose(axis, len(input_size) - 1)\n",
    "\n",
    "    def forward(self, input):  \t\t\t\t# input.shape = (1, seq_len, input_dim)\n",
    "        x = torch.tanh(self.fc1(input))  \t# x.shape = (1, seq_len, dense_dim)\n",
    "        x = self.fc2(x)  \t\t\t\t\t# x.shape = (1, seq_len, attention_hops)\n",
    "        x = self.softmax(x, 1)\n",
    "        attention = x.transpose(1, 2)  \t\t# attention.shape = (1, attention_hops, seq_len)\n",
    "        return attention\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.gcn = GCN()\n",
    "        self.attention = Attention(GCN_OUTPUT_DIM, DENSE_DIM, ATTENTION_HEADS)\n",
    "        self.fc_final = nn.Linear(GCN_OUTPUT_DIM, NUM_CLASSES)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def forward(self, x, adj):  \t\t\t\t\t\t\t\t\t\t\t# x.shape = (seq_len, FEATURE_DIM); adj.shape = (seq_len, seq_len)\n",
    "        x = x.float()\n",
    "        x = self.gcn(x, adj)  \t\t\t\t\t\t\t\t\t\t\t\t# x.shape = (seq_len, GAT_OUTPUT_DIM)\n",
    "\n",
    "        x = x.unsqueeze(0).float()  \t\t\t\t\t\t\t\t\t\t# x.shape = (1, seq_len, GAT_OUTPUT_DIM)\n",
    "        att = self.attention(x)  \t\t\t\t\t\t\t\t\t\t\t# att.shape = (1, ATTENTION_HEADS, seq_len)\n",
    "        node_feature_embedding = att @ x \t\t\t\t\t\t\t\t\t# output.shape = (1, ATTENTION_HEADS, GAT_OUTPUT_DIM)\n",
    "        node_feature_embedding_avg = torch.sum(node_feature_embedding,\n",
    "                                               1) / self.attention.n_heads  # node_feature_embedding_avg.shape = (1, GAT_OUTPUT_DIM)\n",
    "        logits = torch.sigmoid(self.fc_final(node_feature_embedding_avg))  \t# output.shape = (1, NUM_CLASSES)\n",
    "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
    "        Y_prob = torch.softmax(logits, dim = 1)\n",
    "        return logits, Y_hat, Y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "385d89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, inp, out, slope):\n",
    "        \n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = nn.Linear(inp, out, bias=False)\n",
    "        self.a = nn.Linear(out*2, 1, bias=False)\n",
    "        self.leakyrelu = nn.LeakyReLU(slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "    def forward(self, h, adj):\n",
    "        Wh = self.W(h)\n",
    "        Whcat = self.Wh_concat(Wh, adj)\n",
    "        e = self.leakyrelu(self.a(Whcat).squeeze(2))\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = self.softmax(attention)\n",
    "        h_hat = torch.mm(attention, Wh)\n",
    "\n",
    "        return h_hat\n",
    " \n",
    "    def Wh_concat(self, Wh, adj):\n",
    "        N = Wh.size(0)\n",
    "        Whi = Wh.repeat_interleave(N, dim=0)\n",
    "        Whj = Wh.repeat(N, 1)\n",
    "        WhiWhj = torch.cat([Whi, Whj], dim=1)\n",
    "        WhiWhj = WhiWhj.view(N, N, Wh.size(1)*2)\n",
    "\n",
    "        return WhiWhj\n",
    " \n",
    "class MultiHeadGAT(nn.Module):\n",
    "    def __init__(self, inp, out, heads, slope):\n",
    "        super(MultiHeadGAT, self).__init__()\n",
    "        self.attentions = nn.ModuleList([GraphAttentionLayer(inp, out, slope) for _ in range(heads)])\n",
    "        self.tanh = nn.Tanh()\n",
    "  \n",
    "    def forward(self, h, adj):\n",
    "        heads_out = [att(h, adj) for att in self.attentions]\n",
    "        out = torch.stack(heads_out, dim=0).mean(0)\n",
    "    \n",
    "        return self.tanh(out)\n",
    " \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, inp= 2560, out = 256, out2 =64, heads=4, slope=0.01):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = MultiHeadGAT(inp, out, heads, slope)\n",
    "        self.gat2 = MultiHeadGAT(out, out2, heads, slope)\n",
    "        self.attention = Attention(out2, DENSE_DIM, ATTENTION_HEADS)\n",
    "        #self.fc_final = nn.Linear(GCN_OUTPUT_DIM, NUM_CLASSES)\n",
    "        self.fc_final = nn.Linear(out2, NUM_CLASSES)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "  \n",
    "    def forward(self, h, adj):\n",
    "        out = self.gat1(h, adj)\n",
    "        out = self.gat2(out, adj)\n",
    "        att = self.attention(out.unsqueeze(0).float())  \t\t\t\t\t\t\t\t\t\t\t# att.shape = (1, ATTENTION_HEADS, seq_len)\n",
    "        node_feature_embedding = att @ out \t\t\t\t\t\t\t\t\t# output.shape = (1, ATTENTION_HEADS, GAT_OUTPUT_DIM)\n",
    "        node_feature_embedding_avg = torch.sum(node_feature_embedding,\n",
    "                                               1) / self.attention.n_heads  # node_feature_embedding_avg.shape = (1, GAT_OUTPUT_DIM)\n",
    "        logits = torch.sigmoid(self.fc_final(node_feature_embedding_avg))  \t# output.shape = (1, NUM_CLASSES)\n",
    "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
    "        Y_prob = torch.softmax(logits, dim = 1)\n",
    "        return logits, Y_hat, Y_prob\n",
    "        #return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92937142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, epoch):\n",
    "\n",
    "    epoch_loss_train = 0.0\n",
    "    n_batches = 0\n",
    "    for data in tqdm(data_loader):\n",
    "        model.optimizer.zero_grad()\n",
    "        sequence_feature,sequence_graph, labels, sequence_names = data\n",
    "\n",
    "        sequence_feature = torch.squeeze(sequence_feature)\n",
    "        sequence_graph = torch.squeeze(sequence_graph)\n",
    "        if torch.cuda.is_available():\n",
    "            features = Variable(sequence_feature.cuda())\n",
    "            graphs = Variable(sequence_graph.cuda())\n",
    "            y_true = Variable(labels.cuda())\n",
    "        else:\n",
    "            features = Variable(sequence_feature)\n",
    "            graphs = Variable(sequence_graph)\n",
    "            y_true = Variable(labels)\n",
    "\n",
    "        logits,y_pred,Y_hat= model(features, graphs)\n",
    "        #print(logits)\n",
    "        #print(y_pred)\n",
    "        #print(Y_hat)\n",
    "        y_true = y_true.float()\n",
    "        y_pred = y_pred.squeeze(0)\n",
    "        #Y_hat = torch.argmax(y_pred)\n",
    "        #print(y_pred)\n",
    "        #print(logits)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = model.criterion(logits, y_true.to(dtype=torch.long,non_blocking=False))\n",
    "        #l2_lambda = 0.001\n",
    "        #l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        #loss = loss + l2_lambda * l2_norm\n",
    "        #print(loss)\n",
    "        model.optimizer.zero_grad()\n",
    "        #loss.requires_grad = True\n",
    "        loss.backward()\n",
    "       # clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "      #update\n",
    "        model.optimizer.step()\n",
    "\n",
    "        epoch_loss_train += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    epoch_loss_train_avg = epoch_loss_train / n_batches\n",
    "    return epoch_loss_train_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02cfae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    valid_pred = []\n",
    "    valid_true = []\n",
    "    valid_name = []\n",
    "    valid_logit = []\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            sequence_feature,sequence_graph, labels, sequence_names = data\n",
    "\n",
    "            sequence_feature = torch.squeeze(sequence_feature)\n",
    "            sequence_graph = torch.squeeze(sequence_graph)\n",
    "            if torch.cuda.is_available():\n",
    "                features = Variable(sequence_feature.cuda())\n",
    "                graphs = Variable(sequence_graph.cuda())\n",
    "                y_true = Variable(labels.cuda())\n",
    "            else:\n",
    "                features = Variable(sequence_feature)\n",
    "                graphs = Variable(sequence_graph)\n",
    "                y_true = Variable(labels)\n",
    "\n",
    "            logits, y_pred, Y_hat = model(features, graphs)\n",
    "            #logits = logits.mean(dim=0)\n",
    "            #Y_hat = torch.argmax(Y_prob)\n",
    "            y_true = y_true.float()\n",
    "            y_pred = y_pred.squeeze(0)\n",
    "\n",
    "            loss = model.criterion(logits,y_true.to(dtype=torch.long,non_blocking=False))\n",
    "            #l2_lambda = 0.001\n",
    "            #l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            #loss = loss + l2_lambda * l2_norm\n",
    "            #print(loss)\n",
    "                \n",
    "            y_pred = y_pred.cpu().detach().numpy().tolist()\n",
    "            y_true = y_true.cpu().detach().numpy().tolist()\n",
    "            valid_pred.append(y_pred)\n",
    "            valid_true.append(y_true)\n",
    "            valid_name.extend(sequence_names)\n",
    "            valid_logit.append(logits)\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    epoch_loss_avg = epoch_loss / n_batches\n",
    "    print(epoch_loss_avg)\n",
    "    return epoch_loss_avg, valid_true, valid_pred, valid_name, valid_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccdf1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataframe,valid_dataframe,fold=0):\n",
    "    train_loader = DataLoader(dataset=Dataset(dataframe) ,batch_size=1, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(dataset=Dataset(valid_dataframe) ,batch_size=1, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
    "\n",
    "    train_losses = []\n",
    "    train_binary_acc = []\n",
    "\n",
    "    valid_losses = []\n",
    "    valid_binary_acc = []\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(NUMBER_EPOCHS):\n",
    "        print(\"\\n========== Train epoch \" + str(epoch + 1) + \" ==========\")\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss_train_avg = train_one_epoch(model, train_loader, epoch + 1)\n",
    "        print(epoch_loss_train_avg)\n",
    "        #print(\"========== Evaluate Train set ==========\")\n",
    "        #_, train_true, train_pred, _, _ = evaluate(model, train_loader)\n",
    "        #print(len(train_pred), len(train_pred)\n",
    "        #result_train,binpt,bintt = analysis(train_true, train_pred)\n",
    "        #print(\"Train binary acc: \", result_train['binary_acc'])\n",
    "\n",
    "        #train_binary_acc.append(result_train['binary_acc'])\n",
    "        print(\"========== Evaluate Valid set ==========\")\n",
    "        epoch_loss_valid_avg, valid_true, valid_pred, valid_name ,valid_logit= evaluate(model, valid_loader)\n",
    "        result_valid, binp,bint = analysis(valid_true, valid_pred)\n",
    "        print(\"Valid binary acc: \", result_valid['binary_acc'])\n",
    "        valid_binary_acc.append(result_valid['binary_acc'])\n",
    "        if best_val_loss > epoch_loss_valid_avg:\n",
    "            best_val_loss = epoch_loss_valid_avg\n",
    "            best_epoch = epoch + 1\n",
    "            checkpoint = {'state_dict': model.state_dict()}\n",
    "            torch.save(checkpoint,  os.path.join(Model_Path, 'Fold' + str(fold) + '_score_best_model.pkl'))    \n",
    "            #torch.save(model.state_dict(), os.path.join(Model_Path, 'Fold' + str(fold) + '_best_model.pkl'))\n",
    "            valid_detail_dataframe = pd.DataFrame({'names': valid_name, 'stability': valid_true, 'prediction': valid_pred})\n",
    "            valid_detail_dataframe.sort_values(by=['names'], inplace=True)\n",
    "            valid_detail_dataframe.to_csv(Result_Path + 'Fold' + str(fold) + \"_binary_valid_detail.csv\", header=True, sep=',')\n",
    " \n",
    "    #result_all = {\n",
    "     #   'Train_binary_acc': train_binary_acc,\n",
    "     #   'Valid_binary_acc': valid_binary_acc,}\n",
    "    #result = pd.DataFrame(result_all)\n",
    "    #print(\"Fold\", str(fold), \"Best epoch at\", str(best_epoch))\n",
    "    #result.to_csv('result.csv')\n",
    "\n",
    "def analysis(y_true, y_pred):\n",
    "    #print(len(y_pred))\n",
    "    binary_pred = y_pred\n",
    "    #print(binary_pred, y_true)\n",
    "    binary_true= y_true\n",
    "\n",
    "\n",
    "    binary_acc = metrics.accuracy_score(binary_true, binary_pred)\n",
    "    \n",
    "    result = {\n",
    "\n",
    "        'binary_acc': binary_acc,\n",
    "\n",
    "    }\n",
    "    return result , binary_pred, binary_true\n",
    "\n",
    "def cross_validation(all_dataframe,fold_number=10):\n",
    "    print(\"split_seed: \", SEED)\n",
    "    sequence_names = all_dataframe['names'].values\n",
    "    sequence_labels = all_dataframe['stability'].values\n",
    "    kfold = KFold(n_splits=fold_number, shuffle=True)\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, valid_index in kfold.split(sequence_names, sequence_labels):\n",
    "        print(\"\\n========== Fold \" + str(fold + 1) + \" ==========\")\n",
    "        train_dataframe = all_dataframe.iloc[train_index, :]\n",
    "        valid_dataframe = all_dataframe.iloc[valid_index, :]\n",
    "        print(\"Training on\", str(train_dataframe.shape[0]), \"examples, Validation on\", str(valid_dataframe.shape[0]),\n",
    "              \"examples\")\n",
    "        model= Model()\n",
    "\n",
    "        train(model, train_dataframe, valid_dataframe, fold + 1)\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa8eba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_seed:  2333\n",
      "\n",
      "========== Fold 1 ==========\n",
      "Training on 5188 examples, Validation on 1298 examples\n",
      "\n",
      "========== Train epoch 1 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6486/6486 [00:59<00:00, 109.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.475210611320483\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 410.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48435246088012524\n",
      "Valid binary acc:  0.8197226502311248\n",
      "\n",
      "========== Train epoch 2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6486/6486 [01:00<00:00, 106.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4856039162103754\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 408.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47971504244763974\n",
      "Valid binary acc:  0.8204930662557781\n",
      "\n",
      "========== Train epoch 3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6486/6486 [00:59<00:00, 109.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48935707377093535\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 421.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5080279653492987\n",
      "Valid binary acc:  0.7912172573189522\n",
      "\n",
      "========== Train epoch 4 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6486/6486 [01:01<00:00, 106.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49518576868606723\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 400.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.486120246201524\n",
      "Valid binary acc:  0.8204930662557781\n",
      "\n",
      "========== Train epoch 5 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6486/6486 [01:01<00:00, 104.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49612602906263137\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 416.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4931870181406224\n",
      "Valid binary acc:  0.8073959938366718\n",
      "\n",
      "========== Train epoch 6 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6486/6486 [01:01<00:00, 105.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5017467832771157\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 403.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5050909720785997\n",
      "Valid binary acc:  0.8050847457627118\n",
      "\n",
      "========== Train epoch 7 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6486/6486 [01:01<00:00, 105.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499015933090509\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 424.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49706059101917344\n",
      "Valid binary acc:  0.802773497688752\n",
      "\n",
      "========== Train epoch 8 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6486/6486 [01:04<00:00, 100.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5061654080552082\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 389.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5331651047905347\n",
      "Valid binary acc:  0.7596302003081664\n",
      "\n",
      "========== Train epoch 9 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6486/6486 [01:06<00:00, 97.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5067933029436308\n",
      "========== Evaluate Valid set ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1298/1298 [00:03<00:00, 378.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5041512264446779\n",
      "Valid binary acc:  0.7912172573189522\n",
      "\n",
      "========== Train epoch 10 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▍                                                                | 1052/6486 [00:10<00:52, 103.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(all_dataframe, fold_number)\u001b[0m\n\u001b[0;32m     80\u001b[0m     model_s\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     81\u001b[0m     model_s\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(Model_Path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Fold0_best_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m),strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 83\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m fold \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataframe, valid_dataframe, fold)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m========== Train epoch \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ==========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 18\u001b[0m epoch_loss_train_avg \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch_loss_train_avg)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#print(\"========== Evaluate Train set ==========\")\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#_, train_true, train_pred, _, _ = evaluate(model, train_loader)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#print(len(train_pred), len(train_pred)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#train_binary_acc.append(result_train['binary_acc'])\u001b[39;00m\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, data_loader, epoch)\u001b[0m\n\u001b[0;32m     38\u001b[0m   loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     39\u001b[0m  \u001b[38;5;66;03m# clip_grad_norm_(model.parameters(), max_norm=10)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#update\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m   \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m   epoch_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     45\u001b[0m   n_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    103\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[0;32m    110\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_validation(ds,fold_number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "552a805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataframe):\n",
    "    test_loader = DataLoader(dataset=Dataset(test_dataframe) ,batch_size=1, shuffle=False, num_workers=0,collate_fn=collate_fn)\n",
    "    test_result = {}\n",
    "    for model_name in sorted(os.listdir(Model_Path)):\n",
    "        print(model_name)\n",
    "        model_s = Model()\n",
    "        if torch.cuda.is_available():\n",
    "            model_s.cuda()\n",
    "        model_state = torch.load(Model_Path + model_name)['state_dict']\n",
    "        model_s.load_state_dict(model_state, strict=True)\n",
    "        model_s.eval()\n",
    "\n",
    "    epoch_loss_valid_avg, valid_true, valid_pred, valid_name, valid_score = evaluate(model_s, test_loader)\n",
    "    #test_detail_dataframe = pd.DataFrame({'names': valid_name.cpu(), 'target': valid_true.cpu(), 'prediction': valid_pred.cpu(), 'Attention': valid_score.cpu()})\n",
    "\n",
    "    return valid_true,valid_pred,valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cf2915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataframe):\n",
    "    test_loader = DataLoader(dataset=Dataset(test_dataframe) ,batch_size=1, shuffle=False, num_workers=0,collate_fn=collate_fn)\n",
    "    test_result = {}\n",
    "    for model_name in sorted(os.listdir(Model_Path)):\n",
    "        print(model_name)\n",
    "        model_s = Model()\n",
    "        if torch.cuda.is_available():\n",
    "            model_s.cuda()\n",
    "        model_s.load_state_dict(torch.load(Model_Path + model_name),strict=True)\n",
    "        #model_s.load_state_dict(model_state, strict=True)\n",
    "        model_s.eval()\n",
    "\n",
    "    epoch_loss_valid_avg, valid_true, valid_pred, valid_name, valid_score = evaluate(model_s, test_loader)\n",
    "    #test_detail_dataframe = pd.DataFrame({'names': valid_name.cpu(), 'target': valid_true.cpu(), 'prediction': valid_pred.cpu(), 'Attention': valid_score.cpu()})\n",
    "\n",
    "    return valid_true,valid_pred,valid_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
