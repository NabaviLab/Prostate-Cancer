{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85e38a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "#from dataset import Dataset, collate_fn\n",
    "SEED = 2333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c41a8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "#feature_dir = 'Feature_vector//Feature_vector//'\n",
    "graph_dir = 'tg//'\n",
    "label_dir = 'tma_l//'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folders = os.listdir(graph_dir)\n",
    "sequence_features = []\n",
    "sequence_graphs = []\n",
    "labels = []\n",
    "sequence_names=[]\n",
    "for i in range(len(folders)):\n",
    "    sequence_features.append(pickle.load(open(graph_dir + folders[i] +  '//graph.pkl'  , \"rb\"))['node_features'])\n",
    "    sequence_graphs.append(pickle.load(open(graph_dir + folders[i] +  '//graph.pkl'  , \"rb\"))['Adjacency_matrix'])\n",
    "    labels.append(pickle.load(open(label_dir + folders[i] +  '//label.pkl'  , \"rb\"))[0][-1])\n",
    "    sequence_names.append(folders[i])\n",
    "    \n",
    "\n",
    "#vectors = pickle.load(open(feature_dir + folders[i] +  '//vectors.pkl'  , \"rb\"))\n",
    "\n",
    "#labels = pickle.load(open(label_dir + folders[i] +  '//label.pkl'  , \"rb\"))[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff5124a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Path = './tm/'\n",
    "Result_Path = './tm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27552afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MG = dict(zip(sequence_names, sequence_graphs))\n",
    "WF=dict(zip(sequence_names, sequence_features))\n",
    "zipped = list(zip(sequence_names, labels))\n",
    "ds = pd.DataFrame(zipped, columns=['names', 'stability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afa6878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NodeM(name):\n",
    "    return WF[name]\n",
    "def GraphM(name):\n",
    "    return MG[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2471549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ds['names'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a9fa3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "  \n",
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.names = dataframe['names'].values.tolist()\n",
    "        self.labels = dataframe['stability'].values.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sequence_name = self.names[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        sequence_feature = NodeM(sequence_name)\n",
    "\n",
    "        # L * L\n",
    "        sequence_graph = GraphM(sequence_name)\n",
    "        \n",
    "        \n",
    "        sample = {'sequence_feature': sequence_feature,\\\n",
    "                  'sequence_graph': sequence_graph, \\\n",
    "                  'label': label, \\\n",
    "                  'sequence_name': sequence_name, \\\n",
    "                  }\n",
    "        return sample\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequence_feature = []\n",
    "    sequence_graph = []\n",
    "    sequence_names = [] \n",
    "    labels=[]   \n",
    "    for i in range(len(batch)):\n",
    "        sequence_feature.append(batch[i]['sequence_feature'])\n",
    "        sequence_feature=np.asarray(sequence_feature)\n",
    "        sequence_graph.append(batch[i]['sequence_graph'])\n",
    "        sequence_graph=np.asarray(sequence_graph)\n",
    "        sequence_names.append(batch[i]['sequence_name'])\n",
    "        labels.append(batch[i]['label'])\n",
    "        labels= np.asarray(labels)\n",
    "\n",
    "    sequence_feature = torch.from_numpy(sequence_feature).float()\n",
    "    sequence_graph = torch.from_numpy(sequence_graph).float()\n",
    "    labels= torch.from_numpy(labels)\n",
    "\n",
    "    return sequence_feature,sequence_graph, labels, sequence_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bb9fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_EPOCHS = 150\n",
    "LEARNING_RATE = 1E-5\n",
    "WEIGHT_DECAY = 10E-7\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "# GCN parameters\n",
    "GCN_FEATURE_DIM = 2560\n",
    "GCN_HIDDEN_DIM = 512\n",
    "GCN_HIDDEN_DIM1 = 256\n",
    "GCN_HIDDEN_DIM2 = 128\n",
    "GCN_OUTPUT_DIM = 32\n",
    "\n",
    "# Attention parameters\n",
    "DENSE_DIM = 16\n",
    "ATTENTION_HEADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d12ed3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "#from utils.utils import initialize_weights\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ae6fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = input @ self.weight    # X * W\n",
    "        output = adj @ support           # A * X * W\n",
    "        if self.bias is not None:        # A * X * W + b\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(GCN_FEATURE_DIM, GCN_HIDDEN_DIM)\n",
    "        self.ln1 = nn.LayerNorm(GCN_HIDDEN_DIM)\n",
    "        self.gc3 = GraphConvolution(GCN_HIDDEN_DIM, GCN_HIDDEN_DIM1)\n",
    "        self.ln3 = nn.LayerNorm(GCN_HIDDEN_DIM1)\n",
    "        self.gc2 = GraphConvolution(GCN_HIDDEN_DIM1, GCN_OUTPUT_DIM)\n",
    "        self.ln2 = nn.LayerNorm(GCN_OUTPUT_DIM)\n",
    "        self.relu1 = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.relu3 = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.relu4 = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.relu2 = nn.LeakyReLU(0.2,inplace=True)\n",
    "\n",
    "    def forward(self, x, adj):  \t\t\t# x.shape = (seq_len, GCN_FEATURE_DIM); adj.shape = (seq_len, seq_len)\n",
    "        x = self.gc1(x, adj)  \t\t\t\t# x.shape = (seq_len, GCN_HIDDEN_DIM)\n",
    "        x = self.relu1(self.ln1(x))\n",
    "        x = self.gc3(x, adj)  \t\t\t\t# x.shape = (seq_len, GCN_HIDDEN_DIM)\n",
    "        x = self.relu3(self.ln3(x))\n",
    "        x = self.gc2(x, adj)\n",
    "        output = self.relu2(self.ln2(x))\t# output.shape = (seq_len, GCN_OUTPUT_DIM)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, dense_dim, n_heads):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.dense_dim)\n",
    "        self.fc2 = nn.Linear(self.dense_dim, self.n_heads)\n",
    "\n",
    "    def softmax(self, input, axis=1):\n",
    "        input_size = input.size()\n",
    "        trans_input = input.transpose(axis, len(input_size) - 1)\n",
    "        trans_size = trans_input.size()\n",
    "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "        soft_max_2d = torch.softmax(input_2d, dim=1)\n",
    "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "        return soft_max_nd.transpose(axis, len(input_size) - 1)\n",
    "\n",
    "    def forward(self, input):  \t\t\t\t# input.shape = (1, seq_len, input_dim)\n",
    "        x = torch.tanh(self.fc1(input))  \t# x.shape = (1, seq_len, dense_dim)\n",
    "        x = self.fc2(x)  \t\t\t\t\t# x.shape = (1, seq_len, attention_hops)\n",
    "        x = self.softmax(x, 1)\n",
    "        attention = x.transpose(1, 2)  \t\t# attention.shape = (1, attention_hops, seq_len)\n",
    "        return attention\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.gcn = GCN()\n",
    "        self.attention = Attention(GCN_OUTPUT_DIM, DENSE_DIM, ATTENTION_HEADS)\n",
    "        self.fc_final = nn.Linear(GCN_OUTPUT_DIM, NUM_CLASSES)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def forward(self, x, adj):  \t\t\t\t\t\t\t\t\t\t\t# x.shape = (seq_len, FEATURE_DIM); adj.shape = (seq_len, seq_len)\n",
    "        x = x.float()\n",
    "        x = self.gcn(x, adj)  \t\t\t\t\t\t\t\t\t\t\t\t# x.shape = (seq_len, GAT_OUTPUT_DIM)\n",
    "\n",
    "        x = x.unsqueeze(0).float()  \t\t\t\t\t\t\t\t\t\t# x.shape = (1, seq_len, GAT_OUTPUT_DIM)\n",
    "        att = self.attention(x)  \t\t\t\t\t\t\t\t\t\t\t# att.shape = (1, ATTENTION_HEADS, seq_len)\n",
    "        node_feature_embedding = att @ x \t\t\t\t\t\t\t\t\t# output.shape = (1, ATTENTION_HEADS, GAT_OUTPUT_DIM)\n",
    "        node_feature_embedding_avg = torch.sum(node_feature_embedding,\n",
    "                                               1) / self.attention.n_heads  # node_feature_embedding_avg.shape = (1, GAT_OUTPUT_DIM)\n",
    "        logits = torch.sigmoid(self.fc_final(node_feature_embedding_avg))  \t# output.shape = (1, NUM_CLASSES)\n",
    "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
    "        Y_prob = torch.softmax(logits, dim = 1)\n",
    "        return logits, Y_hat, Y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "385d89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, inp, out, slope):\n",
    "        \n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = nn.Linear(inp, out, bias=False)\n",
    "        self.a = nn.Linear(out*2, 1, bias=False)\n",
    "        self.leakyrelu = nn.LeakyReLU(slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "    def forward(self, h, adj):\n",
    "        Wh = self.W(h)\n",
    "        Whcat = self.Wh_concat(Wh, adj)\n",
    "        e = self.leakyrelu(self.a(Whcat).squeeze(2))\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = self.softmax(attention)\n",
    "        h_hat = torch.mm(attention, Wh)\n",
    "\n",
    "        return h_hat\n",
    " \n",
    "    def Wh_concat(self, Wh, adj):\n",
    "        N = Wh.size(0)\n",
    "        Whi = Wh.repeat_interleave(N, dim=0)\n",
    "        Whj = Wh.repeat(N, 1)\n",
    "        WhiWhj = torch.cat([Whi, Whj], dim=1)\n",
    "        WhiWhj = WhiWhj.view(N, N, Wh.size(1)*2)\n",
    "\n",
    "        return WhiWhj\n",
    " \n",
    "class MultiHeadGAT(nn.Module):\n",
    "    def __init__(self, inp, out, heads, slope):\n",
    "        super(MultiHeadGAT, self).__init__()\n",
    "        self.attentions = nn.ModuleList([GraphAttentionLayer(inp, out, slope) for _ in range(heads)])\n",
    "        self.tanh = nn.Tanh()\n",
    "  \n",
    "    def forward(self, h, adj):\n",
    "        heads_out = [att(h, adj) for att in self.attentions]\n",
    "        out = torch.stack(heads_out, dim=0).mean(0)\n",
    "    \n",
    "        return self.tanh(out)\n",
    " \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, inp= 2560, out = 256, out2 =64, heads=4, slope=0.01):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = MultiHeadGAT(inp, out, heads, slope)\n",
    "        self.gat2 = MultiHeadGAT(out, out2, heads, slope)\n",
    "        self.attention = Attention(out2, DENSE_DIM, ATTENTION_HEADS)\n",
    "        #self.fc_final = nn.Linear(GCN_OUTPUT_DIM, NUM_CLASSES)\n",
    "        self.fc_final = nn.Linear(out2, NUM_CLASSES)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "  \n",
    "    def forward(self, h, adj):\n",
    "        out = self.gat1(h, adj)\n",
    "        out = self.gat2(out, adj)\n",
    "        att = self.attention(out.unsqueeze(0).float())  \t\t\t\t\t\t\t\t\t\t\t# att.shape = (1, ATTENTION_HEADS, seq_len)\n",
    "        node_feature_embedding = att @ out \t\t\t\t\t\t\t\t\t# output.shape = (1, ATTENTION_HEADS, GAT_OUTPUT_DIM)\n",
    "        node_feature_embedding_avg = torch.sum(node_feature_embedding,\n",
    "                                               1) / self.attention.n_heads  # node_feature_embedding_avg.shape = (1, GAT_OUTPUT_DIM)\n",
    "        logits = torch.sigmoid(self.fc_final(node_feature_embedding_avg))  \t# output.shape = (1, NUM_CLASSES)\n",
    "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
    "        Y_prob = torch.softmax(logits, dim = 1)\n",
    "        return logits, Y_hat, Y_prob\n",
    "        #return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92937142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, epoch):\n",
    "\n",
    "    epoch_loss_train = 0.0\n",
    "    n_batches = 0\n",
    "    for data in tqdm(data_loader):\n",
    "        model.optimizer.zero_grad()\n",
    "        sequence_feature,sequence_graph, labels, sequence_names = data\n",
    "\n",
    "        sequence_feature = torch.squeeze(sequence_feature)\n",
    "        sequence_graph = torch.squeeze(sequence_graph)\n",
    "        if torch.cuda.is_available():\n",
    "            features = Variable(sequence_feature.cuda())\n",
    "            graphs = Variable(sequence_graph.cuda())\n",
    "            y_true = Variable(labels.cuda())\n",
    "        else:\n",
    "            features = Variable(sequence_feature)\n",
    "            graphs = Variable(sequence_graph)\n",
    "            y_true = Variable(labels)\n",
    "\n",
    "        logits,y_pred,Y_hat= model(features, graphs)\n",
    "        #print(logits)\n",
    "        #print(y_pred)\n",
    "        #print(Y_hat)\n",
    "        y_true = y_true.float()\n",
    "        y_pred = y_pred.squeeze(0)\n",
    "        #Y_hat = torch.argmax(y_pred)\n",
    "        #print(y_pred)\n",
    "        #print(logits)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = model.criterion(logits, y_true.to(dtype=torch.long,non_blocking=False))\n",
    "        #l2_lambda = 0.001\n",
    "        #l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        #loss = loss + l2_lambda * l2_norm\n",
    "        #print(loss)\n",
    "        model.optimizer.zero_grad()\n",
    "        #loss.requires_grad = True\n",
    "        loss.backward()\n",
    "       # clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "      #update\n",
    "        model.optimizer.step()\n",
    "\n",
    "        epoch_loss_train += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    epoch_loss_train_avg = epoch_loss_train / n_batches\n",
    "    return epoch_loss_train_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02cfae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    valid_pred = []\n",
    "    valid_true = []\n",
    "    valid_name = []\n",
    "    valid_logit = []\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            sequence_feature,sequence_graph, labels, sequence_names = data\n",
    "\n",
    "            sequence_feature = torch.squeeze(sequence_feature)\n",
    "            sequence_graph = torch.squeeze(sequence_graph)\n",
    "            if torch.cuda.is_available():\n",
    "                features = Variable(sequence_feature.cuda())\n",
    "                graphs = Variable(sequence_graph.cuda())\n",
    "                y_true = Variable(labels.cuda())\n",
    "            else:\n",
    "                features = Variable(sequence_feature)\n",
    "                graphs = Variable(sequence_graph)\n",
    "                y_true = Variable(labels)\n",
    "\n",
    "            logits, y_pred, Y_hat = model(features, graphs)\n",
    "            #logits = logits.mean(dim=0)\n",
    "            #Y_hat = torch.argmax(Y_prob)\n",
    "            y_true = y_true.float()\n",
    "            y_pred = y_pred.squeeze(0)\n",
    "\n",
    "            loss = model.criterion(logits,y_true.to(dtype=torch.long,non_blocking=False))\n",
    "            #l2_lambda = 0.001\n",
    "            #l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            #loss = loss + l2_lambda * l2_norm\n",
    "            #print(loss)\n",
    "                \n",
    "            y_pred = y_pred.cpu().detach().numpy().tolist()\n",
    "            y_true = y_true.cpu().detach().numpy().tolist()\n",
    "            valid_pred.append(y_pred)\n",
    "            valid_true.append(y_true)\n",
    "            valid_name.extend(sequence_names)\n",
    "            valid_logit.append(logits)\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    epoch_loss_avg = epoch_loss / n_batches\n",
    "    print(epoch_loss_avg)\n",
    "    return epoch_loss_avg, valid_true, valid_pred, valid_name, valid_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccdf1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataframe,valid_dataframe,fold=0):\n",
    "    train_loader = DataLoader(dataset=Dataset(ds) ,batch_size=1, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(dataset=Dataset(valid_dataframe) ,batch_size=1, shuffle=True, num_workers=0,collate_fn=collate_fn)\n",
    "\n",
    "    train_losses = []\n",
    "    train_binary_acc = []\n",
    "\n",
    "    valid_losses = []\n",
    "    valid_binary_acc = []\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(NUMBER_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss_train_avg = train_one_epoch(model, train_loader, epoch + 1)\n",
    "        print(epoch_loss_train_avg)\n",
    "        #print(\"========== Evaluate Train set ==========\")\n",
    "        #_, train_true, train_pred, _, _ = evaluate(model, train_loader)\n",
    "        #print(len(train_pred), len(train_pred)\n",
    "        #result_train,binpt,bintt = analysis(train_true, train_pred)\n",
    "        #print(\"Train binary acc: \", result_train['binary_acc'])\n",
    "\n",
    "        #train_binary_acc.append(result_train['binary_acc'])\n",
    "        epoch_loss_valid_avg, valid_true, valid_pred, valid_name ,valid_logit= evaluate(model, valid_loader)\n",
    "        result_valid, binp,bint = analysis(valid_true, valid_pred)\n",
    "        valid_binary_acc.append(result_valid['binary_acc'])\n",
    "        if best_val_loss > epoch_loss_valid_avg:\n",
    "            best_val_loss = epoch_loss_valid_avg\n",
    "            best_epoch = epoch + 1\n",
    "            checkpoint = {'state_dict': model.state_dict()}\n",
    "            torch.save(checkpoint,  os.path.join(Model_Path, 'Fold' + str(fold) + '_score_best_model.pkl'))    \n",
    "            #torch.save(model.state_dict(), os.path.join(Model_Path, 'Fold' + str(fold) + '_best_model.pkl'))\n",
    "            valid_detail_dataframe = pd.DataFrame({'names': valid_name, 'stability': valid_true, 'prediction': valid_pred})\n",
    "            valid_detail_dataframe.sort_values(by=['names'], inplace=True)\n",
    "            valid_detail_dataframe.to_csv(Result_Path + 'Fold' + str(fold) + \"_binary_valid_detail.csv\", header=True, sep=',')\n",
    " \n",
    "    #result_all = {\n",
    "     #   'Train_binary_acc': train_binary_acc,\n",
    "     #   'Valid_binary_acc': valid_binary_acc,}\n",
    "    #result = pd.DataFrame(result_all)\n",
    "    #print(\"Fold\", str(fold), \"Best epoch at\", str(best_epoch))\n",
    "    #result.to_csv('result.csv')\n",
    "\n",
    "def analysis(y_true, y_pred):\n",
    "    #print(len(y_pred))\n",
    "    binary_pred = y_pred\n",
    "    #print(binary_pred, y_true)\n",
    "    binary_true= y_true\n",
    "\n",
    "\n",
    "    binary_acc = metrics.accuracy_score(binary_true, binary_pred)\n",
    "    \n",
    "    result = {\n",
    "\n",
    "        'binary_acc': binary_acc,\n",
    "\n",
    "    }\n",
    "    return result , binary_pred, binary_true\n",
    "\n",
    "def cross_validation(all_dataframe,fold_number=10):\n",
    "    print(\"split_seed: \", SEED)\n",
    "    sequence_names = all_dataframe['names'].values\n",
    "    sequence_labels = all_dataframe['stability'].values\n",
    "    kfold = KFold(n_splits=fold_number, shuffle=True)\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, valid_index in kfold.split(sequence_names, sequence_labels):\n",
    "        train_dataframe = all_dataframe.iloc[train_index, :]\n",
    "        valid_dataframe = all_dataframe.iloc[valid_index, :]\n",
    "\n",
    "        model= Model()\n",
    "\n",
    "        train(model.cuda(), train_dataframe, valid_dataframe, fold + 1)\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa8eba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_seed:  2333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 244/244 [00:04<00:00, 53.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6875612335126908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 242.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6561667383933554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 244/244 [00:04<00:00, 59.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6479628481825843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 249.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6330140464159908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 244/244 [00:04<00:00, 60.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6353524341935017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 236.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6256182850623617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 244/244 [00:04<00:00, 52.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6283528961119105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 268.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6105992794036865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 244/244 [00:04<00:00, 51.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6183656160948707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 243.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.60545015335083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                        | 25/244 [00:00<00:04, 47.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(all_dataframe, fold_number)\u001b[0m\n\u001b[0;32m     71\u001b[0m valid_dataframe \u001b[38;5;241m=\u001b[39m all_dataframe\u001b[38;5;241m.\u001b[39miloc[valid_index, :]\n\u001b[0;32m     73\u001b[0m model\u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m---> 75\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m fold \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataframe, valid_dataframe, fold)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUMBER_EPOCHS):\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 17\u001b[0m     epoch_loss_train_avg \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(epoch_loss_train_avg)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#print(\"========== Evaluate Train set ==========\")\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#_, train_true, train_pred, _, _ = evaluate(model, train_loader)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#print(len(train_pred), len(train_pred)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m#train_binary_acc.append(result_train['binary_acc'])\u001b[39;00m\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, data_loader, epoch)\u001b[0m\n\u001b[0;32m     25\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#Y_hat = torch.argmax(y_pred)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#print(y_pred)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#print(logits)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#l2_lambda = 0.001\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#loss = loss + l2_lambda * l2_norm\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#print(loss)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2995\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_validation(ds,fold_number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "552a805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataframe):\n",
    "    test_loader = DataLoader(dataset=Dataset(test_dataframe) ,batch_size=1, shuffle=False, num_workers=0,collate_fn=collate_fn)\n",
    "    test_result = {}\n",
    "    for model_name in sorted(os.listdir(Model_Path)):\n",
    "        print(model_name)\n",
    "        model_s = Model()\n",
    "        if torch.cuda.is_available():\n",
    "            model_s.cuda()\n",
    "        model_state = torch.load(Model_Path + model_name)['state_dict']\n",
    "        model_s.load_state_dict(model_state, strict=True)\n",
    "        model_s.eval()\n",
    "\n",
    "    epoch_loss_valid_avg, valid_true, valid_pred, valid_name, valid_score = evaluate(model_s, test_loader)\n",
    "    #test_detail_dataframe = pd.DataFrame({'names': valid_name.cpu(), 'target': valid_true.cpu(), 'prediction': valid_pred.cpu(), 'Attention': valid_score.cpu()})\n",
    "\n",
    "    return valid_true,valid_pred,valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cf2915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataframe):\n",
    "    test_loader = DataLoader(dataset=Dataset(test_dataframe) ,batch_size=1, shuffle=False, num_workers=0,collate_fn=collate_fn)\n",
    "    test_result = {}\n",
    "    for model_name in sorted(os.listdir(Model_Path)):\n",
    "        print(model_name)\n",
    "        model_s = Model()\n",
    "        if torch.cuda.is_available():\n",
    "            model_s.cuda()\n",
    "        model_s.load_state_dict(torch.load(Model_Path + model_name),strict=True)\n",
    "        #model_s.load_state_dict(model_state, strict=True)\n",
    "        model_s.eval()\n",
    "\n",
    "    epoch_loss_valid_avg, valid_true, valid_pred, valid_name, valid_score = evaluate(model_s, test_loader)\n",
    "    #test_detail_dataframe = pd.DataFrame({'names': valid_name.cpu(), 'target': valid_true.cpu(), 'prediction': valid_pred.cpu(), 'Attention': valid_score.cpu()})\n",
    "\n",
    "    return valid_true,valid_pred,valid_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
